# The Google File System
## 1. Motivation
可能是因为google特殊的application需求??? 
还有一个观点是放弃database, 回归file system 
## 2. 做了什么
#### Abstract 
scalable distributed file system; fault tolerance; inexpensive hardware; 性能好
针对google特定的application？  什么特点？？？ 
给distributed应用提供了接口

#### Design overview 
* Assumptions
	* 读: 多stream读， 但是也支持小kb的random read; 写： append
	* concurrency 的问题， 这部分我还真不太懂？？？？ 
	* bandwidth 很重要
* GFS 提供经典的file system的访问interface， 支持传统文件操作 + snapshot 和 record append  （这两个操作具体干什么还不知道？？？）
* Architecture:	
	* master; chunckserver; client
	* file 以chunck存到server中， chunck handle 标识，有replica
	* Master 存metadata, 支持管理整个file system，但是具体干什么我还不太知道？？？ 
	* client 就代表一个application 和 master, server 沟通  但是怎么个流程还不知道？？？ 
	* 关于cache: client 和chunckserver不 cache file (client cache metadata）
* client 一次访存的操作
client只通过master查询server的信息，不通过它访存
	1. client 用file中的chunk index翻译要访问的地址，向master发送request
	2. master 返回client相应的replicas的地址， client cache这些信息
	3. client 选择一个replica所在 server，发送request读
* GFS 选择了一个大的 chunk size 
	出于一些考虑，但是也有disadvantages, 这个分析没有看懂？？？
* Master 中的 Metadata 
	1. 存 file, chunk namspaces; file->chunk的映射; chunk replica 的位置: in-mem 存储
	2. in-mem: 快，并且实际上只需要reasonable的 host mem 支持就可以
	3. chunk replica 的 location 保存: 不persistent存储, 换成异步： heartbeat message向各servers scan 
	4. operation log & checkpoints: 1) log 记录 metadata的修改, 一段时间保存现场，记录checkpoint到disk. 2) 恢复依靠checkpoint和log. 3) checkpoint 生成可以单独开线程 
	4) B-tree 保存 checkpoint   
	B-tree?????   [B树-wiki](https://zh.wikipedia.org/wiki/B树)    [一个对B树的解释](http://blog.csdn.net/v_JULY_v/article/details/6530142)
* consistency model
讲master metadata记录的信息 和 server之间信息的一致性
	* **guarantees** ？？？  对于replicas, cache, component failures等问题的解决    有些不知道它说的是什么？  
	* 应用的启示  appending 替代 overwriting（但是分析没怎么看懂）  同步append，这我也没怎么看懂????  

#### Client, master, chunckserver 之间的交互
* Leases and mutation order 讲什么？？ 
lease 机制（他怎么工作的，真的没看懂？） 在client通过 master 向 replica所在server写时的workflow, 还有一个 primary chunk ？？？？
* Data flow  
数据怎样在primary和chunkserver之间传递的
1. chunkserver以 一个chain来传递数据， primary传给closest的chunkserver, 然后依此类推
2. Pipeline传输，一个chunkserver受到first byte数据就开始往后面的chunkserver传输
* Atomic record appends
1. 类Unix O_append 操作 ?    [还是真的不知道O_append到底怎么做的](http://blog.csdn.net/dog250/article/details/29185821) [Unix文件系统-inode](http://www.ruanyifeng.com/blog/2011/12/inode.html)
2. concurrent 写   这后面说的什么都不知道？？？？？？？
3. 需要的支持logic
4. record append 失败了，怎么做的？？？ 
* Snapshot
copy-on-write???  [介绍](http://www.qnx.com/developers/docs/660/index.jsp?topic=%2Fcom.qnx.doc.neutrino.sys_arch%2Ftopic%2Ffsys_COW_filesystem.html)

我认为是不是应该看一下git的原理
[一个解释](https://git-scm.com/book/zh/v2/Git-内部原理-Git-对象) 

#### Master 需要做的工作
* Namespace Management & Locking 对于一个operation，他操作的对象区域（以绝对路径代替）有一个锁机制去支持多operations的同步执行，对于这个机制的分析还不是很清楚 
* Replica Placement 需要考虑到可靠性&通讯的带宽确定怎么放replica，但是也没说怎么放
* Creation, Re-replication Rebalancing 讲 一个chunk的replica的创建等
 
* Garbage Collection  不在删除的时候整理chunk信息
* Stale Replica Detection  

#### 实现容错 

#### 一些measurements 

#### 实现 GFS 面临的技术挑战

## 3. 相关的问题
#### Intro
主要介绍了Google的一些observation
* fault tolerance(多机器上)一定要做好
* google处理的file很大了
* 对于file的操作是append而不是修改
* 需要在application 和 system 支持API的实现上折中，例如把consistency支持放到哪
#### Related work

## 4. 总结
## Ref
[paper](http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)