# Google File system
characters: 1. different considerations 2. component failure 3. huge file 4. data appending way 5. API for concurrency 
## Introduction
* Large files with new data appending to it

## Design Overview
* Assumptions
	* Appending mainly 
	* Support concurrently appending to a file 
* Interface
	* usual operations like create, delete, open, close ... 
	* More: snapshot, record append(concurrent append to a file)
* Architecture
	* client -> master/ chunkservers
	* Files divided into fixed-size chunks 
	* Master store metatdata and is responsible for management things 
* Single Master
	* Own global information, but do not join read/ write operations
* Chunk Size
	* Choose a large size for reasons
* Metadata in master 
	* file and chunk namespaces(in mem and also logs in disk); files->chunks; chunk replicas' locations(not persistently)
	* chunk locations: poll through HeartBeat messages 
	* Operation log: persistent
* Consistency Model 
	* consistency: same data in each replica; defined: concurrent mutation can see the changes reflected by its entirety

## System Interactions
* Lease and Mutation Order
	* To ensure mutations accomplished on all replicas and with order  
	* Primary chunk own the lease, and decide the serilization of mutations
	* Push data and control to write are seperated 
* Data Flow
	* Strategies for transfer data
* Atomic Record Appends
	* Concurrent record append also follows the lease work flow
	* Primary decides the append order by counting exact appending offsets
* Snapshot 
	* Revoke leases for all chunks need to be copied
	* Then copy-on-write

## Master Operation
* Namespace Management and Locking  (create, snapshot operations)
	* Maintain a lookup table: pathname->metadata  (Some techs for compression)
	* Read-write lock  to allow multiple ops at the same time but in proper serialization
* Replica Placement
	* Spread chunk replicas across racks
* Creation, Re-replication, Rebalancing
	* Following principles to create replicated chunks
	* Re-repl facing failures
	* Rebalance for load balance
* Garbage Collection (after a file is deleted)
	* master log the deletion -> master rename to hide file -> master delete it while it's not-recovered for days
	* After remove file, the master remove this files's metadata for chunks -> master scan not useful chunks(not reachable from any file) -> Exchange this with chunkserver -> chunkserver deletes those replicas  
* Stale Replica Detection 

## Fault Tolerance and Diagnosis
## Measurements 
## Experiences
## Related work 
## Conclusionsy